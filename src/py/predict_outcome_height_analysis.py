import sys
import os 
from scipy import ndimage

import math
import pandas as pd
import numpy as np 
import seaborn as sns

import torch
import pdb
import matplotlib.pyplot as plt
import SimpleITK as sitk

def read_data(dataframe, mount_point):
  # read output generated by process_heights script
  # faster to read processed heights instead of creating them everytime

  list_class = []
  list_files = []
  list_heights = []
  list_x = []
  for idx, row in dataframe.iterrows():
    seg_path = os.path.join(mount_point, row['seg'])
    npy = seg_path.replace('seg', 'segmentation_heights_v2').replace('.nrrd', '.npy')
    if os.path.exists(npy):
      data = np.load(npy,allow_pickle=True).item()
      x_values = data['x_bottom']
      heights = data['heights']

      valid_indices = ~np.isnan(heights)
      heights = heights[valid_indices]
      x_values = x_values[valid_indices]

      xmin = np.min(x_values)
      xmax = np.max(x_values)

      new_x = np.linspace(xmin, xmax, num=400)
      new_y = np.interp(new_x, x_values, heights)
      list_heights.append(new_y)
      list_x.append(new_x)

      list_class.append(row['class'])
      list_files.append(npy)

  all_heights = np.stack(list_heights, axis=0)
  all_x = np.stack(list_x, axis=0)
  all_classes = np.array(list_class)

  return all_heights, all_x, all_classes, list_files

def compute_eye_bbx(seg, label=1, pad=0.01):

  shape = seg.shape
  
  ij = torch.argwhere(seg.squeeze() != 0)

  bb = torch.tensor([0, 0, 0, 0])# xmin, ymin, xmax, ymax

  bb[0] = torch.clip(torch.min(ij[:,1]) - shape[1]*pad, 0, shape[1])
  bb[1] = torch.clip(torch.min(ij[:,0]) - shape[0]*pad, 0, shape[0])
  bb[2] = torch.clip(torch.max(ij[:,1]) + shape[1]*pad, 0, shape[1])
  bb[3] = torch.clip(torch.max(ij[:,0]) + shape[0]*pad, 0, shape[0])
  
  return bb

def get_outcome(outcome, val):
  outcome.append(val)
  return outcome

def global_curve_analysis(ratio, outcome, cpt_count):

  # global_mean = np.mean(ratio[50:350]) ## first labels
  global_mean = np.mean(ratio) ## second labels

  if global_mean < 0.09:
    outcome = get_outcome(outcome, 'entropion G') 
    cpt_count.append(1)
  elif global_mean > 0.125:
    outcome = get_outcome(outcome, 'overcorrection G')
    cpt_count.append(-1)
  else:
    cpt_count.append(0)
    outcome = get_outcome(outcome, 'Healthy')
  
  return outcome, cpt_count

def middle_side_analysis(ratio, outcome, cpt_count):
  # comparing height between left and right of middle section -> it should be similar.

  val_left = ratio[100:150]
  val_right = ratio[250:300]

  # # if the left has an appropriate correction
  if  np.mean(val_left) < 0.10:
    outcome = get_outcome(outcome, 'entropion L')
    cpt_count.append(1)

  elif np.mean(val_left) > 0.15:
    outcome = get_outcome(outcome, 'overcorrection L')
    cpt_count.append(-1)

  elif  np.mean(val_right) < 0.10:
    outcome = get_outcome(outcome, 'entropion R')
    cpt_count.append(1)

  elif np.mean(val_right) > 0.15:
    outcome = get_outcome(outcome, 'overcorrection R')
    cpt_count.append(-1)

  if (0.12 < np.mean(val_left) < 0.14): 
    if np.mean(val_right) < 0.10:
      # if right to low it's undercorrected
      outcome = get_outcome(outcome, 'entropion R')
      cpt_count.append(1)
    elif np.mean(val_right) > 0.14:
      # if right to high it's overcorrected
      outcome = get_outcome(outcome, 'overcorrection R')
      cpt_count.append(-1)

    else:
      cpt_count.append(0)
      outcome = get_outcome(outcome, 'Healthy')

      
  # if the right has an appropriate correction
  elif (0.12 < np.mean(val_right) < 0.14): 
    if np.mean(val_left) < 0.10:
      # if right to low it's undercorrected
      outcome = get_outcome(outcome, 'entropion L')
      cpt_count.append(1)

    elif np.mean(val_left) > 0.14:
      # if right to high it's overcorrected
      outcome = get_outcome(outcome, 'overcorrection L')
      cpt_count.append(-1)

    else:
      cpt_count.append(0)
      outcome = get_outcome(outcome, 'Healthy')


  else:
    cpt_count.append(0)

  return outcome, cpt_count

def mean_analysis(mean_df, outcome, cpt_count):
  
  mean_left = mean_df.iloc[0]['mean']
  mean_middle = mean_df.iloc[1]['mean']
  mean_right =  mean_df.iloc[2]['mean']
  len_cpt = len(cpt_count)

  if ( mean_middle < mean_left) or (mean_middle < mean_right):
    if mean_middle > 0.11:
      cpt_count.append(-1)
      if mean_middle < mean_left:
        outcome = get_outcome(outcome, 'overcorrection L')
      if mean_middle < mean_right:
        outcome = get_outcome(outcome, 'overcorrection R')
    else:
      ### if the sides mean are higher than the middle correction --> the middle section is probably undercorrected
      outcome = get_outcome(outcome, 'entropion M')
      cpt_count.append(1)

  ## Here we're checking in more details the height differences between the middle and the side section.
  diff_left =  np.abs(mean_left-mean_middle) / mean_middle
  diff_right =  np.abs(mean_right-mean_middle) / mean_middle

  # if similar correction for middle and side
  if diff_left <= 0.1 and diff_right <= 0.1:
    if mean_middle > 0.12:
      outcome = get_outcome(outcome, 'overcorrection R')
      outcome = get_outcome(outcome, 'overcorrection L')
      cpt_count.append(-1)

    elif mean_middle < 0.12:
      outcome = get_outcome(outcome, 'entropion M')
      cpt_count.append(1)

  if diff_left < 0.1 and diff_right > 0.3:
    outcome = get_outcome(outcome, "overcorrection L")
    cpt_count.append(-1)

  elif diff_right < 0.1 and diff_left > 0.3:
    outcome = get_outcome(outcome, "overcorrection R")
    cpt_count.append(-1)

  if diff_left > 0.40:
    if mean_left < 0.06:
      outcome = get_outcome(outcome, 'entropion L')

      cpt_count.append(1)

    if mean_middle > 0.12:
      outcome = get_outcome(outcome, 'overcorrection M')
      cpt_count.append(-1)

  elif diff_right > 0.40:
    if mean_right < 0.06:
      outcome = get_outcome(outcome, 'entropion R')

      cpt_count.append(1)

    if mean_middle > 0.12:
      outcome = get_outcome(outcome, 'overcorrection M')

      cpt_count.append(-1)

  if len_cpt == len(cpt_count):
    cpt_count.append(0)

  return outcome, cpt_count

def max_analysis(dataframe, outcome, cpt_count):
  maxs = dataframe['max'].tolist()
  if maxs[1] > 0.145  :
    outcome = get_outcome(outcome, 'overcorrection M')
    cpt_count.append(-1)

  if maxs[0] > 0.145:
    outcome = get_outcome(outcome, 'overcorrection L')
    cpt_count.append(-1)

  if maxs[2] > 0.145:
    outcome = get_outcome(outcome, 'overcorrection R')
    cpt_count.append(-1)
  
  if maxs[0] < 13 and maxs[1] < 13 and maxs[2] < 13:
    cpt_count.append(0)

  return outcome, cpt_count

def adjust_classes_probabilities_based(cpt_count):
  mean_count =  np.mean(cpt_count)
  argmax = np.argmax(cpt_count)

  class_adjusted = 0
  if mean_count < 0:
    p = np.abs(mean_count)
    class_adjusted = 2
    if p <=0.4:
      class_adjusted = 0

  elif mean_count > 0 :
    p = np.abs(mean_count)
    class_adjusted = 1
    if p <= 0.2:
      class_adjusted = 0
  else:
    class_adjusted = 0
    p=1.0
  return class_adjusted, argmax, np.round(p,2)

def separate_good_from_bad(outcome):
  dic_outcome = {'entropion':[], 'overcorrection':[]}
  locations = []
  for elt in outcome :
    elts = elt.split(' ')
    if 'entropion' in elts[0]:
      dic_outcome[elts[0]].append(elts[1])
      locations.append(elts[1])
    elif 'overcorrection' in elts[0]:
      dic_outcome[elts[0]].append(elts[1])
      locations.append(elts[1])

  new_dict = {k: list(np.unique(v)) for k, v in dic_outcome.items() if v} 

  return new_dict, np.unique(locations)

def find_wavy_eyelid(smooth_ratio,outcome):
  gradient = np.gradient(smooth_ratio)
  gradient = gradient[30:-30]
  abs_grad = np.abs(gradient)
  thr = np.mean(abs_grad) + 2*np.std(abs_grad)
  if thr > 0.0010:

    peaks = abs_grad > np.mean(abs_grad)+ 2*np.std(abs_grad)
    original_idx_peaks = np.where(peaks)[0]
    idx_peaks_to_keep = np.where(np.diff(original_idx_peaks) > 1)
    if len(original_idx_peaks) > 0:
      global_localization = original_idx_peaks[idx_peaks_to_keep].tolist()
      global_localization.append(original_idx_peaks[-1])
      num_peaks = len(global_localization)      
      if num_peaks == 2:
        if thr > 0.0017:
          return "Yes"
        else:
          return "Maybe"

      elif num_peaks >=2:
        return "Yes"
      else:
        return 'No'

def find_locations(dict_outcome):
    all_keys=list(dict_outcome.keys())
    if len(all_keys) == 1:
      # one outcome
      vals = dict_outcome[all_keys[0]]
      if 'G' in vals: vals.remove('G')
      locations = {all_keys[0]: vals}
      
    else:

      vals = list(dict_outcome.values())
      list1, list2 = vals[0], vals[1]
      if 'G' in list1: list1.remove('G')
      if 'G' in list2: list2.remove('G')

      overlap = set(list1) & set(list2)
      if overlap:
        for val in overlap:
          while val in list1:
              list1.remove(val)
          while val in list2:
            list2.remove(val)
        locations = {k: list(np.unique(v)) for k, v in dict_outcome.items() if v} 
        return locations
      else:
        locations =  {all_keys[0]:list1, all_keys[1]:list2 }
    return locations




df_train = pd.read_csv('/CMF/data/lumargot/trachoma/mtss_seg.csv')

seg_column='seg'
class_column='class'
mount_point='/CMF/data/lumargot/trachoma/PoPP_Data/mtss'
drop_labels=['Reject', 'Short Incision']
concat_labels=['overcorrection', 'Gap', 'ECA', 'Fleshy']
df_train = df_train.loc[df_train['drop']==0]

df_all_labels = pd.read_csv('/CMF/data/lumargot/trachoma/csv_mtss_pret/csv_updated/mtss_pret_combined.csv')
# df_all_labels = pd.read_csv('/CMF/data/lumargot/trachoma/PoPP_Data/mtss/csv/set_062025/new_annotations_mtss.csv')
df_all_labels = df_all_labels.loc[df_all_labels['class']!=3]
df_all_labels = df_all_labels.loc[df_all_labels['to_drop']==0]

if drop_labels is not None:
  df_all_labels = df_all_labels[ ~ df_all_labels['label'].isin(drop_labels)]

if concat_labels is not None:
    replacement_val = df_all_labels.loc[ df_all_labels['label'] == concat_labels[0]]['class'].unique()
    df_all_labels.loc[ df_all_labels['label'].isin(concat_labels), 'class' ] = replacement_val[0]

unique_classes = sorted(df_all_labels['class'].unique())
class_mapping = {value: idx for idx, value in enumerate(unique_classes)}

df_all_labels['class'] = df_all_labels['class'].map(class_mapping)
df_all_labels['name_img'] = df_all_labels['filename'].apply(lambda filename: os.path.basename(filename))

drop = []
new_outcome = []
for idx, row in df_train.iterrows():
  img_name = os.path.basename(row['img'])

  all_labels_subjects = df_all_labels.loc[df_all_labels['name_img'] == img_name]

  if len(all_labels_subjects)> 0:
    drop.append(0)
    unique_labels = np.unique(all_labels_subjects['class'], return_counts=False)

    if len(unique_labels) == 2:
      if 0 in unique_labels:
        outcome = max(unique_labels)
      else: #conflict
        outcome = 4
    elif len(unique_labels) == 3: #conflict 
      outcome = 4
    else:
      outcome = unique_labels[0]

    new_outcome.append(outcome)
  else:
    drop.append(1)
    new_outcome.append('na')

df_train['drop'] = drop
df_train['new_label'] = new_outcome
df_train = df_train.loc[df_train['drop']==0]
df_train = df_train.loc[df_train['new_label']!='na']


all_heights_train, all_x_train, _, files = read_data(df_train, mount_point)

l_gt = []
l_classes = []
l_predicted = []
l_locations = []
l_wavy = []
l_proba = []
l_cid = []
l_seg = []
l_eye = []

# for IDX in range(100):
for IDX in range(len(all_heights_train)):

  name_seg = os.path.join('seg', os.path.basename(files[IDX])).replace('.npy', '.nrrd')

  cid, eye = os.path.splitext(os.path.basename(files[IDX]))[0].split('_12Intra_')
  eye = eye.split('_postop')[0]


  row = df_train.loc[ df_train['seg'] == name_seg]
  class_name = row['new_label'].item()

  img = os.path.join(mount_point, row['img'].item())
  seg = os.path.join(mount_point, name_seg)

  img= sitk.GetArrayFromImage(sitk.ReadImage(img)).copy()
  seg_t = torch.tensor(sitk.GetArrayFromImage(sitk.ReadImage(seg)).copy())
  bb = compute_eye_bbx(seg_t)


  smooth_distances = ndimage.gaussian_filter1d(all_heights_train[IDX], sigma=4)

  real_length = all_x_train[IDX][-1] - all_x_train[IDX][0]
  array_lenght = len(all_x_train[IDX])
  ratio = all_heights_train[IDX] /real_length
  smooth_ratio = smooth_distances /real_length

  index = np.arange(len(ratio))
  bins = pd.cut(index, bins=[0,100,300,400], right=False,
                labels=["0-100", "100-300", "300-400"])

  count_df = pd.DataFrame({"bin": bins, "value": ratio})

  mean_per_bin = count_df.groupby("bin",observed=False)["value"].mean().reset_index(name="mean")
  max_per_bin = count_df.groupby("bin",observed=False)["value"].max().reset_index(name="max")
  
  merged = mean_per_bin.merge(max_per_bin, on="bin")

  mean_max_df = merged.melt(id_vars="bin", value_vars=["mean", "max"], var_name="stat", value_name="value")

  cpt_count = []
  outcome = []
  outcome, cpt_count = global_curve_analysis(ratio, outcome, cpt_count)
  outcome, cpt_count = middle_side_analysis(ratio, outcome, cpt_count)
  outcome, cpt_count = mean_analysis(mean_per_bin, outcome, cpt_count)
  outcome, cpt_count = max_analysis(merged, outcome, cpt_count)

  if outcome == None :
    outcome = get_outcome(outcome, 'Healthy')
    cpt_count.append(0)

  isWavy = find_wavy_eyelid(smooth_ratio,outcome)
  if isWavy == 'Yes': 
    if np.mean(smooth_ratio) > 0.12:
      cpt_count.append(-1)
    elif np.mean(smooth_ratio) < 0.10:  
      cpt_count.append(1)
  class_adjusted, max_val, p = adjust_classes_probabilities_based(cpt_count)

  dict_outcome, unique_locations  = separate_good_from_bad(outcome)

  if class_adjusted > 0:
    locations = find_locations(dict_outcome)
    # colored_mask = color_mask(seg_t, locations)
  else:
    locations = 'N/A'
    colored_mask = seg_t

  l_classes.append(class_name)
  l_predicted.append(class_adjusted)
  l_locations.append(locations)
  l_wavy.append(isWavy)
  l_proba.append(p)
  l_seg.append(name_seg)
  l_cid.append(cid)
  l_eye.append(eye)


  # plt.figure(figsize=(12,5))
  # plt.subplot(221)
  # plt.title(f'cropped image: {name_seg}')
  # plt.imshow(img[bb[1]:bb[3],bb[0]:bb[2]])

  # if len(np.unique(seg_t)) == 2: ## contain only eyelid 
  #   image_mask = seg_t.float()
      
  # else:# if contain entire eye seg_tmented
  #   image_mask = (seg_t == 3).float()
  # image_mask = image_mask[bb[1]:bb[3],bb[0]:bb[2]]
  # plt.subplot(222)
  # plt.title('cropped eyelid segmentation')
  # plt.imshow(image_mask)
  # plt.title(f'annotation : {class_name}')

  # plt.subplot(223)
  # plt.title(f"predicted: {class_adjusted} with p: {p} \n is Wavy: {isWavy}")
  # plot_section_on_images(locations, image_mask, img[bb[1]:bb[3],bb[0]:bb[2]])
  # plt.subplot(224)
  # plt.plot(np.arange(ratio.shape[0]), ratio)
  # plt.title('height ratio')

  # plt.savefig(f'example_{IDX}_predicted_class{class_name}.png')


df_out = pd.DataFrame(data={'cid': l_cid,
                            'eye':l_eye,
                            'seg_path':l_seg,
                            'gt':l_classes, 
                            'pred':l_predicted,
                            'locations':l_locations,
                            'isWavy':l_wavy,
                            'probability':l_proba,
                            })


df_out.to_csv('mtss_eyelid_prediction_for_analysis.csv')