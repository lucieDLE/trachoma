{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366183dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lumargot/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of Albumentations is available: '2.0.6' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/lumargot/trachoma/src/py')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "from torchvision.ops import nms\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from visualization import *\n",
    "\n",
    "from utils import *\n",
    "from nets.segmentation import FasterRCNN\n",
    "from loaders.tt_dataset import TTDatasetBX,TTDataModuleBX, BBXImageTrainTransform, BBXImageEvalTransform, BBXImageTestTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3293999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/CMF/data/lumargot/trachoma/\"\n",
    "\n",
    "df_train = pd.read_csv('/CMF/data/lumargot/trachoma/csv_updated/mtss_pret_combined_train_fold0_train_train.csv')\n",
    "df_val = pd.read_csv('/CMF/data/lumargot/trachoma/csv_updated/mtss_pret_combined_train_fold0_train_test.csv')\n",
    "# df_train = pd.read_csv('/CMF/data/lumargot/trachoma/csv_updated/mtss_pret_combined_train_fold0_train_train.csv')\n",
    "\n",
    "concat_labels=['overcorrection', 'ECA', 'Gap', 'Fleshy']\n",
    "drop_labels = ['Short Incision', 'Reject']\n",
    "\n",
    "img_column= \"filename\" \n",
    "class_column = 'class'\n",
    "label_column = 'label'\n",
    "\n",
    "map ={ 1:'Healthy', 2:'Entropion', 3:'Overcorrection'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce254a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = remove_labels(df_train, class_column, label_column, drop_labels=drop_labels, concat_labels=concat_labels)\n",
    "df_val = remove_labels(df_val, class_column, label_column, drop_labels=drop_labels, concat_labels=concat_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a72a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttdata = TTDataModuleBX(df_train, df_val, df_train, batch_size=16, num_workers=2, img_column='filename',severity_column='sev', \n",
    "                        mount_point=mount_point, class_column= class_column,\n",
    "                        train_transform=BBXImageTrainTransform(), valid_transform=BBXImageEvalTransform(), test_transform=BBXImageTestTransform())\n",
    "ttdata.setup()\n",
    "dataload = ttdata.val_dataloader()\n",
    "ds = ttdata.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c493d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBalancedBoxBatcher:\n",
    "    def __init__(self, batch_size=16):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function that balances classes in a batch\"\"\"\n",
    "        images = torch.stack([item['img'] for item in batch])\n",
    "       \n",
    "        masks = torch.stack([item['mask'] for item in batch])\n",
    "\n",
    "        original_boxes = [item['boxes'] for item in batch]\n",
    "        original_labels = [item['labels'] for item in batch]\n",
    "        \n",
    "        # Count boxes per class across the entire batch\n",
    "        all_labels = []\n",
    "        for item in batch:\n",
    "            all_labels.append(item['labels'])\n",
    "        \n",
    "        # Find minimum count across classes\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        classes, counts = torch.unique(all_labels, return_counts=True)\n",
    "        min_count = counts.min().item()\n",
    "        \n",
    "        targets = []\n",
    "        for i, (boxes, labels, mask) in enumerate(zip(original_boxes, original_labels, masks)):\n",
    "            \n",
    "            img_classes = torch.unique(labels)\n",
    "            \n",
    "            # For each class in this image, sample boxes\n",
    "            img_balanced_boxes, img_balanced_labels = [], []\n",
    "\n",
    "            for cls in img_classes:\n",
    "                mask = (labels == cls)\n",
    "                cls_boxes = boxes[mask]\n",
    "                cls_labels = labels[mask]\n",
    "                \n",
    "                # Calculate how many to keep of this class\n",
    "                # (divide min_count proportionally among images)\n",
    "\n",
    "                proportion = len(cls_boxes) / counts[classes == cls].item()\n",
    "                keep_count = max(1, int(min_count * proportion))\n",
    "                keep_count = min(keep_count, len(cls_boxes))\n",
    "                \n",
    "                # Randomly sample\n",
    "                if len(cls_boxes) > keep_count:\n",
    "                    indices = torch.randperm(len(cls_boxes))[:keep_count]\n",
    "                    cls_boxes = cls_boxes[indices]\n",
    "                    cls_labels = cls_labels[indices]\n",
    "                \n",
    "                img_balanced_boxes.append(cls_boxes)\n",
    "                img_balanced_labels.append(cls_labels)\n",
    "            \n",
    "            if img_balanced_boxes:\n",
    "                img_boxes = torch.cat(img_balanced_boxes)\n",
    "                img_labels = torch.cat(img_balanced_labels)\n",
    "            else:\n",
    "                img_boxes = boxes\n",
    "                img_labels = labels\n",
    "            \n",
    "            dic_i = {'labels': img_labels, \n",
    "                     'boxes': img_boxes,\n",
    "                     'mask': mask}\n",
    "            targets.append(dic_i)\n",
    "\n",
    "        return images, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fbdb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TTDatasetBX(df_train, transform=BBXImageTrainTransform(),img_column='filename', \n",
    "                        mount_point=mount_point, class_column= class_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SimpleBalancedBoxBatcher(batch_size=16)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 16,\n",
    "    num_workers=0,\n",
    "    collate_fn=sampler.collate_fn\n",
    ")\n",
    "\n",
    "for IDX, batch in enumerate(tqdm(loader)):\n",
    "    # targets = batch\n",
    "    imgs, targets = batch\n",
    "    # imgs = targets.pop('img', None)\n",
    "\n",
    "    plt.figure(figsize=(10,20))\n",
    "    for j in range(len(targets)):\n",
    "        img = imgs[j].permute(1,2,0) \n",
    "        boxes = targets[j]['boxes']\n",
    "        labels = targets[j]['labels']\n",
    "        plt.subplot(8,2,j+1)\n",
    "        ax = plt.gca()\n",
    "        ax.imshow(img)\n",
    "        for k in range(labels.shape[0]):\n",
    "            box = boxes[k]\n",
    "            label = labels[k]\n",
    "            x1, y1, x2, y2 = box\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            if label == 1: color = 'green'\n",
    "            elif label == 2: color = 'blue'\n",
    "            else: color = 'cyan'\n",
    "            rect = Rectangle((x1, y1), width, height, fill=False, color=color, linewidth=1.5)\n",
    "            ax.add_patch(rect)\n",
    "    if IDX == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX, batch in enumerate(tqdm(dataload)):\n",
    "    imgs, targets = batch\n",
    "    labels = [t['labels'] for t in targets]\n",
    "    classes, counts = torch.unique(torch.cat(labels), return_counts=True)\n",
    "    print(counts)\n",
    "    # plt.figure(figsize=(10,20))\n",
    "    # for j in range(len(targets)):\n",
    "    #     img = imgs[j].permute(1,2,0) \n",
    "    #     boxes = targets[j]['boxes']\n",
    "    #     labels = targets[j]['labels']\n",
    "\n",
    "    #     plt.subplot(8,2,j+1)\n",
    "    #     ax = plt.gca()\n",
    "    #     ax.imshow(img)\n",
    "\n",
    "    #     for k in range(labels.shape[0]):\n",
    "    #         box = boxes[k]\n",
    "    #         label = labels[k]\n",
    "\n",
    "    #         x1, y1, x2, y2 = box\n",
    "    #         width, height = x2 - x1, y2 - y1\n",
    "\n",
    "    #         if label == 1: color = 'green'\n",
    "    #         elif label == 2: color = 'blue'\n",
    "    #         else: color = 'cyan'\n",
    "\n",
    "    #         rect = Rectangle((x1, y1), width, height, fill=False, color=color, linewidth=1.5)\n",
    "    #         ax.add_patch(rect)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flyby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
